{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QFaTCNEYl5W"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt  \n",
        "import numpy as np\n",
        "import csv\n",
        "import sys\n",
        "import random\n",
        "from random import seed\n",
        "from random import randint\n",
        "from sklearn.metrics import accuracy_score\n",
        "from bs4 import BeautifulSoup\n",
        "from random import randrange\n",
        "from scipy.special import expit\n",
        "from scipy.special import softmax\n",
        "\n",
        "np.set_printoptions(precision=10)\n",
        "pd.option_context('display.max_rows', None, 'display.max_columns', None)  # more options can be specified also\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "seed(1)\n",
        "\n",
        "np.set_printoptions(precision=10, )\n",
        "pd.options.display.float_format = '{:.2f}'.format\n",
        "\n",
        "%precision 10\n",
        "\n",
        "!cp -r /content/drive/MyDrive/10601_DataSets/HW8 /content"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlNA5EmZaUTK"
      },
      "source": [
        "def Print_DF(df):\n",
        "  with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
        "    print(df)\n",
        "\n",
        "def read_csv_files(path):\n",
        "  df_read = pd.read_csv(path, quoting=csv.QUOTE_NONE, encoding='utf-8', sep='\\n', header=None, keep_default_na=False)\n",
        "\n",
        "  # Expand all the elements of maze into multiple dataframe columns\n",
        "  df_read = df_read[0].apply(lambda x: pd.Series(list(x)))\n",
        "\n",
        "  print(\"Maze shape: \", df_read.shape)\n",
        "  print(\"Maze is :\")\n",
        "  print(df_read)\n",
        "\n",
        "  return df_read"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_xAWU1Mrr_v"
      },
      "source": [
        "# maze_description.values.tolist()\n",
        "# # Reverse the row index in DF\n",
        "# maze_df = maze_df.iloc[::-1]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1-k-U11gvMA"
      },
      "source": [
        "Set_Of_Actions = {}\n",
        "Set_Of_Actions['W'] = 0 \n",
        "Set_Of_Actions['N'] =  1\n",
        "Set_Of_Actions['E'] =  2\n",
        "Set_Of_Actions['S'] =  3"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLzNux_SR6aW"
      },
      "source": [
        "def Get_Transition_Probability(maze_df, Set_Of_Actions):\n",
        "  ''' \n",
        "  Get transition probability along with all state action pairs with all possible transitions\n",
        "  \n",
        "  Warning: Probability transition function isn't generic and it might not behave well.\n",
        "  The results differ from the main solution; i still don't understant what it meant by saying: \n",
        "  \"The transitions are deterministic, so if the agent chooses to move left, the next state will \n",
        "  be the grid to the left of the previous one.\". I think this is the main issue in my understanding.\n",
        "\n",
        "  However this function is just to demonstrate the ability to perform transitions. It's still ill\n",
        "  defined though and needs adjustments. It's mainly for demonstration purposes.\n",
        "  '''\n",
        "\n",
        "  # Get height and width of the maze\n",
        "  maze_height =  maze_df.shape[0]\n",
        "  maze_width = maze_df.shape[1] \n",
        "\n",
        "  # Initiate Bellman's equation by intializaing 3 matrices: P, R, V\n",
        "  # Transition probability matrix: P\n",
        "  P = np.zeros((maze_height * maze_width, maze_height * maze_width))\n",
        "\n",
        "  # Dict for valid and invalid state transitions for a single state\n",
        "  transitions = {}\n",
        "\n",
        "  # Dict for all coordinates that one can go to from the current state\n",
        "  all_coordinates = {}\n",
        "  \n",
        "  # Flag to indicate that the next state is a terminate state\n",
        "  terminate_flag = False\n",
        "\n",
        "  # List of invalid states to remove later from P matrix\n",
        "  invalid = []\n",
        "\n",
        "  # Remeber the state action decisions made\n",
        "  state_action = {}\n",
        "\n",
        "  # Loop over all rows in the maze\n",
        "  for x, row in maze_df.iterrows():\n",
        "\n",
        "    # Loop over all columns in each row in the maze\n",
        "    for y, value in enumerate(row):\n",
        "      # For a single value, check to see if it's whether an obstacle or it's the end\n",
        "      # of the maze or it's a state that we need to handle\n",
        "      # Obstacles aren't states, skip them\n",
        "\n",
        "      if value == '*' or value == 'G':\n",
        "        row_index = x * maze_width + y\n",
        "        invalid.append(row_index)\n",
        "        continue\n",
        "            \n",
        "      # This is a state, let's get its transition probability\n",
        "      # First get its coordinates in (x, y) tuple\n",
        "      x_state, y_state = x, y\n",
        "      # Get its North coordinate\n",
        "      all_coordinates['N'] =  x - 1, y\n",
        "      # Get its south coordinate\n",
        "      all_coordinates['S'] = x + 1, y\n",
        "      # Get its East coordinate\n",
        "      all_coordinates['E'] = x, y + 1\n",
        "      # Get its west coordinate\n",
        "      all_coordinates['W'] =  x, y - 1\n",
        "\n",
        "      for action_name in ['N', 'S', 'E', 'W']:\n",
        "        x_tmp, y_tmp = all_coordinates[action_name]\n",
        "        if x_tmp == -1 or y_tmp == -1 or x_tmp == maze_height or y_tmp == maze_width:\n",
        "          state_action[((x_state, y_state), action_name)] = 'bounce'\n",
        "          x_tmp, y_tmp = 0, 0\n",
        "        else:\n",
        "          transitions[action_name] = (x_tmp, y_tmp)\n",
        "          state_action[((x_state, y_state), action_name)] = (x_tmp, y_tmp)\n",
        "          \n",
        "      available_directions = transitions.copy()\n",
        "      \n",
        "      # Now that we have all possible transitions, check to see if there is \n",
        "      # an obstacle in the next state or that we are already at the Goal\n",
        "      for direction in transitions.keys():\n",
        "        x_next, y_next = transitions[direction]      \n",
        "\n",
        "        # DataFrames are accessed by columns first, so use y value to select\n",
        "        # the column, the the x value to select the row to loc the element  \n",
        "        if maze_df[y_next][x_next] == '*':\n",
        "          available_directions.pop(direction)\n",
        "          state_action[((x_state, y_state), direction)] = 'bounce'\n",
        "\n",
        "        if maze_df[y_next][x_next] == 'G':\n",
        "          available_directions.pop(direction)\n",
        "          terminate_flag = True\n",
        "          state_action[((x_state, y_state), direction)] = 'termination'\n",
        "          break\n",
        "      \n",
        "      # Check to see if the current state has terminatation as next state\n",
        "      if terminate_flag:\n",
        "        terminate_flag = False\n",
        "        continue\n",
        "\n",
        "      # Empty directions means the next state is the goal state, in this case \n",
        "      # terminate the current state with P transistion to be all zeros.\n",
        "      if not (available_directions):\n",
        "        # The matrix is already initialized with zeros, so just continue\n",
        "        continue\n",
        "\n",
        "      # Now we have at least one direction, but multiple directions are valid too.\n",
        "      # So, we need to prioritize the direction for us to move -> N, E, S, W\n",
        "      prioriry_dirs = ['N', 'E', 'S', 'W']\n",
        "      for dir in prioriry_dirs:\n",
        "        location = available_directions.get(dir, -1)\n",
        "        if location == -1:\n",
        "          continue\n",
        "\n",
        "        x_next, y_next = location\n",
        "        row_index = x_state * maze_width + y_state\n",
        "        col_index = x_next * maze_width + y_next\n",
        "        print(\"action taken at state {} is {}\".format((x_state, y_state), dir))\n",
        "        P[row_index][col_index] = 1\n",
        "\n",
        "        # That's the state we need, put 1 in the transition and just break the loop\n",
        "        break\n",
        "\n",
        "  P = np.delete(P, invalid, axis=0)\n",
        "  P = np.delete(P, invalid, axis=1)\n",
        "  return P, state_action"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgiEI5jdnCBq"
      },
      "source": [
        "def value_iteration(Set_Of_Actions, epochs, Discount_factor, path):\n",
        "  '''\n",
        "  This is a planning method used when the model is  known. Under assumption \n",
        "  that we have access to: Set of states,  Set of Actions,\n",
        "  Transition probabililty T(s, a, s'), Reward function R(s, a, s').\n",
        "\n",
        "  This function implements the value iteration algorithm. Initialize V(s) = 0 for\n",
        "  each state s that's not occupied by and obstacle.\n",
        "  Don't model V(s) when s is occupied by an obstacle.\n",
        "\n",
        "  This is \"synchronous\" version from value iteration\n",
        "  '''\n",
        "  \n",
        "  maze_df = read_csv_files(path)\n",
        "  \n",
        "  maze_end = (0, maze_df.shape[1] - 1)\n",
        "  maze_start = (maze_df.shape[0] - 1, 0)\n",
        "  \n",
        "  # Get transition probability along with all state action pairs with all possible transitions\n",
        "  \n",
        "  # Warning: Probability transition function isn't generic and it might not behave well.\n",
        "  # The results differ from the main solution; i still don't understant what it meant by saying: \n",
        "  # \"The transitions are deterministic, so if the agent chooses to move left, the next state will \n",
        "  # be the grid to the left of the previous one.\". I think this is the main issue in my understanding.\n",
        "\n",
        "  # However this function is just to demonstrate the ability to perform transitions. It's still ill\n",
        "  # defined though and needs adjustments\n",
        "  P, state_action_pair = Get_Transition_Probability(maze_df, Set_Of_Actions)\n",
        "\n",
        "  R = np.ones((P.shape[0], 1)) * -1\n",
        "  \n",
        "  V = np.zeros((P.shape[0], 1))\n",
        "\n",
        "  # Value iterations are merely applying bellman's equations iteratively\n",
        "  for _ in range(epochs):\n",
        "    V = R + Discount_factor * np.dot(P, V)\n",
        "\n",
        "  # Get all possible states\n",
        "  states = []\n",
        "  for key in state_action_pair.keys():\n",
        "    if key[0] not in states:\n",
        "      states.append(key[0])\n",
        "\n",
        "  # Get Q values for all state action pairs\n",
        "  Q_values = {}\n",
        "  for idx, state in enumerate(states):\n",
        "    for action in Set_Of_Actions:\n",
        "      SAPair = (state, action)\n",
        "      action = state_action_pair.get(SAPair, -1)\n",
        "      if action == 'bounce':\n",
        "        Q_values[SAPair] = -1 + Discount_factor * V[idx]\n",
        "      \n",
        "      elif action == 'termination':\n",
        "        Q_values[SAPair] = -1 + Discount_factor * 0\n",
        "      \n",
        "      else:\n",
        "        Q_values[SAPair] = -1 + Discount_factor * np.squeeze(V[states.index(action)])\n",
        "\n",
        "  # Get policy values using max Q value obtained at each state\n",
        "  policy_state = {}\n",
        "  for idx, state in enumerate(states):\n",
        "    policy_state[state] = 0\n",
        "    max_num = -float('inf')\n",
        "    for action in Set_Of_Actions:\n",
        "      SAPair = (state, action)\n",
        "      qvalue = Q_values[SAPair]\n",
        "\n",
        "      if qvalue > max_num:\n",
        "        max_num = qvalue\n",
        "        policy_state[state] = action\n",
        "\n",
        "      if qvalue == max_num:\n",
        "        tmp_state = policy_state[state]\n",
        "        if Set_Of_Actions[tmp_state] >= Set_Of_Actions[action]:\n",
        "          policy_state[state] = tmp_state\n",
        "  \n",
        "  print(state_action_pair)\n",
        "  print(\"V values: \\n\", V)\n",
        "  print(\"Q values: \\n\", Q_values)\n",
        "  print(\"Policy values: \\n\", policy_state)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sv9oJX4v83xx",
        "outputId": "5512ce62-424a-4018-9718-447a9be2f211"
      },
      "source": [
        "value_iteration(Set_Of_Actions, 1000, 0.9, \n",
        "                '/content/HW8/maze1.txt')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maze shape:  (3, 5)\n",
            "Maze is :\n",
            "   0  1  2  3  4\n",
            "0  *  *  .  *  G\n",
            "1  .  .  .  *  .\n",
            "2  S  *  .  .  .\n",
            "action taken at state (0, 2) is S\n",
            "action taken at state (1, 0) is E\n",
            "action taken at state (1, 1) is E\n",
            "action taken at state (1, 2) is N\n",
            "action taken at state (2, 0) is N\n",
            "action taken at state (2, 2) is N\n",
            "action taken at state (2, 3) is E\n",
            "action taken at state (2, 4) is N\n",
            "{((0, 2), 'N'): 'bounce', ((0, 2), 'S'): (1, 2), ((0, 2), 'E'): 'bounce', ((0, 2), 'W'): 'bounce', ((1, 0), 'N'): 'bounce', ((1, 0), 'S'): (2, 0), ((1, 0), 'E'): (1, 1), ((1, 0), 'W'): 'bounce', ((1, 1), 'N'): 'bounce', ((1, 1), 'S'): 'bounce', ((1, 1), 'E'): (1, 2), ((1, 1), 'W'): (1, 0), ((1, 2), 'N'): (0, 2), ((1, 2), 'S'): (2, 2), ((1, 2), 'E'): 'bounce', ((1, 2), 'W'): (1, 1), ((1, 4), 'N'): 'termination', ((1, 4), 'S'): (2, 4), ((1, 4), 'E'): 'bounce', ((1, 4), 'W'): 'bounce', ((2, 0), 'N'): (1, 0), ((2, 0), 'S'): 'bounce', ((2, 0), 'E'): 'bounce', ((2, 0), 'W'): 'bounce', ((2, 2), 'N'): (1, 2), ((2, 2), 'S'): 'bounce', ((2, 2), 'E'): (2, 3), ((2, 2), 'W'): 'bounce', ((2, 3), 'N'): 'bounce', ((2, 3), 'S'): 'bounce', ((2, 3), 'E'): (2, 4), ((2, 3), 'W'): (2, 2), ((2, 4), 'N'): (1, 4), ((2, 4), 'S'): 'bounce', ((2, 4), 'E'): 'bounce', ((2, 4), 'W'): (2, 3)}\n",
            "V values: \n",
            " [[-10.  ]\n",
            " [-10.  ]\n",
            " [-10.  ]\n",
            " [-10.  ]\n",
            " [ -1.  ]\n",
            " [-10.  ]\n",
            " [-10.  ]\n",
            " [ -2.71]\n",
            " [ -1.9 ]]\n",
            "Q values: \n",
            " {((0, 2), 'W'): array([-10.]), ((0, 2), 'N'): array([-10.]), ((0, 2), 'E'): array([-10.]), ((0, 2), 'S'): -9.999999999999995, ((1, 0), 'W'): array([-10.]), ((1, 0), 'N'): array([-10.]), ((1, 0), 'E'): -9.999999999999995, ((1, 0), 'S'): -9.999999999999995, ((1, 1), 'W'): -9.999999999999995, ((1, 1), 'N'): array([-10.]), ((1, 1), 'E'): -9.999999999999995, ((1, 1), 'S'): array([-10.]), ((1, 2), 'W'): -9.999999999999995, ((1, 2), 'N'): -9.999999999999995, ((1, 2), 'E'): array([-10.]), ((1, 2), 'S'): -9.999999999999995, ((1, 4), 'W'): array([-1.9]), ((1, 4), 'N'): -1.0, ((1, 4), 'E'): array([-1.9]), ((1, 4), 'S'): -2.71, ((2, 0), 'W'): array([-10.]), ((2, 0), 'N'): -9.999999999999995, ((2, 0), 'E'): array([-10.]), ((2, 0), 'S'): array([-10.]), ((2, 2), 'W'): array([-10.]), ((2, 2), 'N'): -9.999999999999995, ((2, 2), 'E'): -3.439, ((2, 2), 'S'): array([-10.]), ((2, 3), 'W'): -9.999999999999995, ((2, 3), 'N'): array([-3.439]), ((2, 3), 'E'): -2.71, ((2, 3), 'S'): array([-3.439]), ((2, 4), 'W'): -3.439, ((2, 4), 'N'): -1.9, ((2, 4), 'E'): array([-2.71]), ((2, 4), 'S'): array([-2.71])}\n",
            "Policy values: \n",
            " {(0, 2): 'W', (1, 0): 'W', (1, 1): 'W', (1, 2): 'W', (1, 4): 'N', (2, 0): 'W', (2, 2): 'E', (2, 3): 'E', (2, 4): 'N'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhhBvBPyAdLo"
      },
      "source": [
        "class Environment:\n",
        "  def __init__(self, filename):\n",
        "    '''\n",
        "    The environment returns the following three values when the \n",
        "    agent takes an action a at state s:\n",
        "    next state: the state of the agent after taking action a at state s\n",
        "    reward: the reward received from taking action a at state s\n",
        "    is terminal: an integer value indicating whether the agent reaches a terminal state after\n",
        "    taking action a at state s. The value is 1 if terminal state is reached and 0 otherwise.\n",
        "    '''\n",
        "    maze = read_csv_files(filename)\n",
        "\n",
        "    self.maze_df = maze\n",
        "\n",
        "    self.maze_end = (0, maze.shape[1] - 1)\n",
        "    self.maze_start = (maze.shape[0] - 1, 0)\n",
        "\n",
        "    self.maze_height = maze.shape[0]\n",
        "    self.maze_width = maze.shape[1]\n",
        "\n",
        "    maze_list = self.maze_df.values.tolist()\n",
        "    total_states = 1\n",
        "    \n",
        "    for l in maze_list:\n",
        "      total_states += l.count('.')\n",
        "\n",
        "    self.x, self.y = self.maze_start\n",
        "\n",
        "    self.N_action = (-1, 0)\n",
        "    self.S_action = (1, 0)\n",
        "    self.E_action = (0, 1)\n",
        "    self.W_action = (0, -1)\n",
        "\n",
        "    # N, S, E, W actions\n",
        "    self.List_of_actions = [1, 3, 2, 0]\n",
        "    self.List_of_actions_names = ['N', 'S', 'E', 'W']\n",
        "    self.List_f_actions_coordinates = [self.N_action, self.S_action, self.E_action, self.W_action]\n",
        "\n",
        "  def step(self, a):\n",
        "    '''\n",
        "    This function takes in an action a, simulates a step, \n",
        "    sets the current state to the next state,\n",
        "    and returns next state, reward, is terminal.\n",
        "    '''\n",
        "    index = 0\n",
        "    for index in range(len(self.List_of_actions)):\n",
        "      if a == self.List_of_actions[index]:\n",
        "        break\n",
        "    \n",
        "    x_transit, y_transit = self.List_f_actions_coordinates[index]\n",
        "\n",
        "    x_state = self.x + x_transit\n",
        "    y_state = self.y + y_transit\n",
        "\n",
        "    if 0 <= x_state < self.maze_height and 0 <= y_state < self.maze_width and self.maze_df[y_state][x_state] != \"*\":\n",
        "      self.x = x_state\n",
        "      self.y = y_state\n",
        "    else:\n",
        "      pass\n",
        "    \n",
        "    is_terminal = (self.x, self.y) == self.maze_end\n",
        "    reward = -1\n",
        "\n",
        "    return self.x, self.y, reward, is_terminal\n",
        "\n",
        "  def reset(self):\n",
        "    '''\n",
        "    This function resets the agent state to the initial \n",
        "    state and returns the initial state.\n",
        "    '''\n",
        "    self.x, self.y = self.maze_start"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AfD_VQGpX3p"
      },
      "source": [
        "def init_q_table(Q_table, env):\n",
        "  maze_df = env.maze_df\n",
        "  for x, row in maze_df.iterrows():\n",
        "    for y, state in enumerate(row):\n",
        "      # Start state and termination states have values in the table\n",
        "      # Obstacles aren't states\n",
        "      if state == '*':\n",
        "        continue\n",
        "      x_state, y_state = x, y\n",
        "      Q_table[(x_state, y_state)] = [0 for a in env.List_of_actions_names]\n",
        "  \n",
        "def Q_learning_Algorithm(path, Q_table, number_of_epochs, max_length, alpha, gamma, eps):\n",
        "\n",
        "  environment = Environment(path)\n",
        "\n",
        "  # Initialization for Q table with zeros\n",
        "  init_q_table(Q_table, environment)\n",
        "\n",
        "  # for t = 0 to T\n",
        "  for _ in range(number_of_epochs):\n",
        "    # At the beginning of every new epoch, reset the environment\n",
        "    # SelectState() -> s = start state\n",
        "    environment.reset()\n",
        "    \n",
        "    # For each step of epoch t\n",
        "    for _ in range(max_length):\n",
        "      # Get the current state\n",
        "      x_state, y_state = environment.x, environment.y\n",
        "\n",
        "      # Expolit vs expolre choice. Decide based on Epsilon value (Eps-greedy algorithm)\n",
        "      # With prob (1 - Eps) select a = max a' Q(s, a') -> exploit\n",
        "      # With prob (Eps) select a = random action from Set of actions A -> explore\n",
        "      p = np.random.random()\n",
        "      \n",
        "      # Explore\n",
        "      if p <= eps:\n",
        "        action = np.random.randint(0, 3)\n",
        "      \n",
        "      # Exploit\n",
        "      else:\n",
        "        action = np.argmax(Q_table[(x_state, y_state)])\n",
        "      \n",
        "      # Get reward and Next state\n",
        "      new_x, new_y, r, terminated = environment.step(action)\n",
        "\n",
        "      # Apply the equation: Q(s, a) <- (1 - alpha) * Q(s, a) + alpha * [r' + gamma * max a' Q(s', a') - Q(s, a)]\n",
        "      \n",
        "      # First get the max a' for Q(s', a')\n",
        "      action_dash = np.argmax(Q_table[(new_x, new_y)])    \n",
        "      \n",
        "      # Note: This equation is for non-deterministic environment\n",
        "      Q_table[(x_state, y_state)][action] = (1 - alpha) * Q_table[(x_state, y_state)][action] + alpha * (r + gamma * Q_table[(new_x, new_y)][action_dash])\n",
        "\n",
        "      if terminated:\n",
        "        break\n",
        "\n",
        "  return Q_table "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Finrz-XuZ6r",
        "outputId": "5bf98c97-1ce6-4f71-a1cc-5523f1e41fb8"
      },
      "source": [
        "# Dictionary of state-action pairs. It includes all actions for all states\n",
        "# keys -> (x, y) coordenates of state, value -> list of actions\n",
        "Q_table = {}\n",
        "maze_file = '/content/HW8/maze1.txt'\n",
        "Q_table = Q_learning_Algorithm(maze_file, Q_table, 2000, 20, 0.8, 0.9, 0.05)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maze shape:  (3, 5)\n",
            "Maze is :\n",
            "   0  1  2  3  4\n",
            "0  *  *  .  *  G\n",
            "1  .  .  .  *  .\n",
            "2  S  *  .  .  .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZTl3BVT00jh",
        "outputId": "01c37f80-82df-4a14-cde5-a2252d18c26a"
      },
      "source": [
        "value = []\n",
        "policy = []\n",
        "\n",
        "for key in Q_table.keys():\n",
        "  best_action = np.argmax(Q_table[key])\n",
        "  value.append([key, Q_table[key][best_action]])\n",
        "  policy.append([key, best_action])\n",
        "\n",
        "print(value)\n",
        "print(Q_table)\n",
        "print(policy)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[(0, 2), -4.68559], [(0, 4), 0], [(1, 0), -5.217031], [(1, 1), -4.68559], [(1, 2), -4.0951], [(1, 4), -1.0], [(2, 0), -5.6953279000000006], [(2, 2), -3.439], [(2, 3), -2.71], [(2, 4), -1.9]]\n",
            "{(0, 2): [-5.205093495556215, -4.957066724822221, -5.147185001813629, -4.68559], (0, 4): [0, 0, 0, 0], (1, 0): [-5.6953279000000006, -5.6953279000000006, -5.217031, -5.472880287187667], (1, 1): [-5.6953279000000006, -5.217031, -4.68559, -4.867811268624385], (1, 2): [-5.217031, -5.217031, -4.68559, -4.0951], (1, 4): [-1.9, -1.0, -1.9, -1.51424], (2, 0): [-6.12579511, -5.6953279000000006, -6.12579511, -6.003626221142586], (2, 2): [-4.0951, -4.68559, -3.439, -3.9364499865600004], (2, 3): [-4.0951, -3.439, -2.71, -2.8360704000000005], (2, 4): [-3.439, -1.9, -2.71, -2.2131200000000004]}\n",
            "[[(0, 2), 3], [(0, 4), 0], [(1, 0), 2], [(1, 1), 2], [(1, 2), 3], [(1, 4), 1], [(2, 0), 1], [(2, 2), 2], [(2, 3), 2], [(2, 4), 1]]\n"
          ]
        }
      ]
    }
  ]
}